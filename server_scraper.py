import os
import time
import random
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

BASE_URL = "https://www.perspectivebd.com/"  # üîÅ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶≤ URL ‡¶¶‡¶ø‡¶®
DOWNLOAD_DIR = "downloaded_site"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

visited_urls = set()  # URL ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡¶¨‡ßÉ‡¶§‡ßç‡¶§‡¶ø ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá

# ‡¶è‡¶á ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶¨‡ßá
EXCLUDED_FOLDERS = ['vendor', 'vendors', 'node_modules', 'assets/vendor', 'js/vendor', 'css/vendor', 'storage/logs','storage/framework', 'storage/debugbar',  ]

def sanitize_filename(filename):
    """‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡¶¨‡ßà‡¶ß ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶®‡ßã"""
    # Windows/Linux ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶¨‡ßà‡¶ß ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶®‡ßã
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶°‡¶ü ‡¶∏‡¶∞‡¶æ‡¶®‡ßã (‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∂‡ßá‡¶∑‡ßá‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶∞‡¶æ‡¶ñ‡¶æ)
    if filename.count('.') > 1:
        parts = filename.split('.')
        filename = '_'.join(parts[:-1]) + '.' + parts[-1]
    return filename

def get_filename_from_url(url):
    """URL ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path = parsed.path
    
    # ‡¶Ø‡¶¶‡¶ø ‡¶™‡¶æ‡¶• '/' ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßá‡¶∑ ‡¶π‡¶Ø‡¶º ‡¶§‡¶æ‡¶π‡¶≤‡ßá index.html ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
    if path.endswith('/') or path == '':
        return 'index.html'
    
    # ‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
    filename = os.path.basename(path)
    
    # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá ‡¶è‡¶¨‡¶Ç ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßá‡¶ú ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá .html ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
    if '.' not in filename and not filename.endswith('/'):
        filename += '.html'
    
    return sanitize_filename(filename)

def create_directory_structure(url, base_dir):
    """URL ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path_parts = [part for part in parsed.path.split('/') if part]
    
    current_dir = base_dir
    # ‡¶∂‡ßá‡¶∑ ‡¶Ö‡¶Ç‡¶∂ ‡¶¨‡¶æ‡¶¶‡ßá ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶Ö‡¶Ç‡¶∂‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
    for part in path_parts[:-1]:
        current_dir = os.path.join(current_dir, sanitize_filename(part))
        os.makedirs(current_dir, exist_ok=True)
    
    return current_dir

def download_file(url, dest_path):
    """‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ"""
    try:
        print(f"‚¨áÔ∏è Downloading: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"‚úÖ Downloaded: {dest_path}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to download {url}: {e}")
        return False

def should_exclude_url(url):
    """URL ‡¶ü‡¶ø ‡¶è‡¶ï‡ßç‡¶∏‡¶ï‡ßç‡¶≤‡ßÅ‡¶° ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    
    for excluded in EXCLUDED_FOLDERS:
        if f'/{excluded}/' in path or path.endswith(f'/{excluded}') or path.startswith(f'/{excluded}/'):
            return True
    return False

def download(url, dest_dir):
    """‡¶Æ‡ßÇ‡¶≤ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®"""
    # URL ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶≠‡¶ø‡¶ú‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
    if url in visited_urls:
        return
    
    # vendor ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶®
    if should_exclude_url(url):
        print(f"‚è≠Ô∏è Skipping vendor folder: {url}")
        return
    
    visited_urls.add(url)
    print(f"üîç Scanning: {url}")
    
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error accessing {url}: {e}")
        return

    # HTML ‡¶™‡ßá‡¶ú ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
    dir_path = create_directory_structure(url, dest_dir)
    filename = get_filename_from_url(url)
    file_path = os.path.join(dir_path, filename)
    
    if not os.path.exists(file_path):
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(r.text)
            print(f"üíæ Saved HTML: {file_path}")
        except Exception as e:
            print(f"‚ùå Failed to save HTML {url}: {e}")

    # HTML ‡¶™‡¶æ‡¶∞‡ßç‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
    soup = BeautifulSoup(r.text, "html.parser")
    
    # ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
    resources = []
    
    # CSS ‡¶´‡¶æ‡¶á‡¶≤
    for link in soup.find_all('link', href=True):
        if link.get('rel') and 'stylesheet' in link.get('rel'):
            resources.append(link['href'])
    
    # JavaScript ‡¶´‡¶æ‡¶á‡¶≤
    for script in soup.find_all('script', src=True):
        resources.append(script['src'])
    
    # ‡¶õ‡¶¨‡¶ø
    for img in soup.find_all('img', src=True):
        resources.append(img['src'])
    
    # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶≤‡¶ø‡¶Ç‡¶ï
    for link in soup.find_all('a', href=True):
        href = link['href']
        # ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶è‡¶ï‡¶á ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶®‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï
        full_url = urljoin(url, href)
        if BASE_URL in full_url and full_url not in visited_urls:
            resources.append(href)
    
    # ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
    for resource in resources:
        if not resource or resource in ['/', '#', 'javascript:void(0)']:
            continue
            
        full_resource_url = urljoin(url, resource)
        
        # ‡¶è‡¶ï‡¶á ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶® ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
        if not full_resource_url.startswith(BASE_URL):
            continue
        
        # vendor ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶®
        if should_exclude_url(full_resource_url):
            print(f"‚è≠Ô∏è Skipping vendor resource: {full_resource_url}")
            continue
        
        # ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        resource_dir = create_directory_structure(full_resource_url, dest_dir)
        resource_filename = get_filename_from_url(full_resource_url)
        resource_path = os.path.join(resource_dir, resource_filename)
        
        if not os.path.exists(resource_path):
            if download_file(full_resource_url, resource_path):
                # ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶Æ ‡¶¨‡¶ø‡¶∞‡¶§‡¶ø
                delay = random.uniform(1.5, 4.0)
                print(f"‚è±Ô∏è Sleeping for {delay:.2f} seconds...")
                time.sleep(delay)
        
        # ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø HTML ‡¶™‡ßá‡¶ú ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∞‡¶ø‡¶ï‡¶æ‡¶∞‡ßç‡¶∏‡¶ø‡¶≠‡¶≠‡¶æ‡¶¨‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        if (resource_filename.endswith('.html') or 
            resource_filename == 'index.html' or 
            '.' not in resource_filename):
            download(full_resource_url, dest_dir)

# ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
print(f"üöÄ Starting download from: {BASE_URL}")
print(f"üìÅ Download directory: {DOWNLOAD_DIR}")
download(BASE_URL, DOWNLOAD_DIR)
print("üéâ Download completed!")