import os
import time
import random
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

BASE_URL = "https://www.perspectivebd.com/"  # üîÅ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶≤ URL ‡¶¶‡¶ø‡¶®
DOWNLOAD_DIR = "downloaded_site"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

visited_urls = set()  # URL ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡¶¨‡ßÉ‡¶§‡ßç‡¶§‡¶ø ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá

# ‡¶è‡¶á ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶¨‡ßá
EXCLUDED_FOLDERS = ['vendor', 'vendors', 'node_modules', 'assets/vendor', 'js/vendor', 'css/vendor']

def sanitize_filename(filename):
    """‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡¶¨‡ßà‡¶ß ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶®‡ßã"""
    # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤‡¶ø‡¶Ç
    if filename.startswith('.') and len(filename) > 1:
        # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶Ö‡¶¨‡ßà‡¶ß ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶®
        clean_name = re.sub(r'[<>:"/\\|?*]', '_', filename[1:])  # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶°‡¶ü ‡¶¨‡¶æ‡¶¶‡ßá
        return '.' + clean_name
    
    # Windows/Linux ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶¨‡ßà‡¶ß ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶®‡ßã
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶°‡¶ü ‡¶∏‡¶∞‡¶æ‡¶®‡ßã (‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∂‡ßá‡¶∑‡ßá‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶∞‡¶æ‡¶ñ‡¶æ)
    if filename.count('.') > 1:
        parts = filename.split('.')
        filename = '_'.join(parts[:-1]) + '.' + parts[-1]
    return filename

def get_filename_from_url(url):
    """URL ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path = parsed.path
    
    # ‡¶Ø‡¶¶‡¶ø ‡¶™‡¶æ‡¶• '/' ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßá‡¶∑ ‡¶π‡¶Ø‡¶º ‡¶§‡¶æ‡¶π‡¶≤‡ßá index.html ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
    if path.endswith('/') or path == '':
        return 'index.html'
    
    # ‡¶´‡¶æ‡¶á‡¶≤‡¶®‡ßá‡¶Æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
    filename = os.path.basename(path)
    
    # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤ ‡¶ö‡ßá‡¶ï (.‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ ‡¶´‡¶æ‡¶á‡¶≤)
    if filename.startswith('.') and len(filename) > 1:
        return sanitize_filename(filename)  # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßá ‡¶§‡ßá‡¶Æ‡¶®‡¶á ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®
    
    # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá ‡¶è‡¶¨‡¶Ç ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßá‡¶ú ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá .html ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
    if '.' not in filename and not filename.endswith('/'):
        filename += '.html'
    
    return sanitize_filename(filename)

def create_directory_structure(url, base_dir):
    """URL ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path_parts = [part for part in parsed.path.split('/') if part]
    
    current_dir = base_dir
    # ‡¶∂‡ßá‡¶∑ ‡¶Ö‡¶Ç‡¶∂ ‡¶¨‡¶æ‡¶¶‡ßá ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶Ö‡¶Ç‡¶∂‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
    for part in path_parts[:-1]:
        current_dir = os.path.join(current_dir, sanitize_filename(part))
        os.makedirs(current_dir, exist_ok=True)
    
    return current_dir

def download_file(url, dest_path):
    """‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ"""
    try:
        print(f"‚¨áÔ∏è Downloading: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"‚úÖ Downloaded: {dest_path}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to download {url}: {e}")
        return False

def should_exclude_url(url):
    """URL ‡¶ü‡¶ø ‡¶è‡¶ï‡ßç‡¶∏‡¶ï‡ßç‡¶≤‡ßÅ‡¶° ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    
    for excluded in EXCLUDED_FOLDERS:
        if f'/{excluded}/' in path or path.endswith(f'/{excluded}') or path.startswith(f'/{excluded}/'):
            return True
    return False

def download(url, dest_dir):
    """‡¶Æ‡ßÇ‡¶≤ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®"""
    # URL ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶≠‡¶ø‡¶ú‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
    if url in visited_urls:
        return
    
    # vendor ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶®
    if should_exclude_url(url):
        print(f"‚è≠Ô∏è Skipping vendor folder: {url}")
        return
    
    visited_urls.add(url)
    print(f"üîç Scanning: {url}")
    
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error accessing {url}: {e}")
        return

    # HTML ‡¶™‡ßá‡¶ú ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
    dir_path = create_directory_structure(url, dest_dir)
    filename = get_filename_from_url(url)
    file_path = os.path.join(dir_path, filename)
    
    if not os.path.exists(file_path):
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(r.text)
            print(f"üíæ Saved HTML: {file_path}")
        except Exception as e:
            print(f"‚ùå Failed to save HTML {url}: {e}")

    # HTML ‡¶™‡¶æ‡¶∞‡ßç‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
    soup = BeautifulSoup(r.text, "html.parser")
    
    # ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
    resources = []
    
    # CSS ‡¶´‡¶æ‡¶á‡¶≤
    for link in soup.find_all('link', href=True):
        if link.get('rel') and 'stylesheet' in link.get('rel'):
            resources.append(link['href'])
    
    # JavaScript ‡¶´‡¶æ‡¶á‡¶≤
    for script in soup.find_all('script', src=True):
        resources.append(script['src'])
    
    # ‡¶õ‡¶¨‡¶ø
    for img in soup.find_all('img', src=True):
        resources.append(img['src'])
    
    # ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶´‡¶æ‡¶á‡¶≤
    for video in soup.find_all('video'):
        if video.get('src'):
            resources.append(video['src'])
        for source in video.find_all('source', src=True):
            resources.append(source['src'])
    
    # ‡¶Ö‡¶°‡¶ø‡¶ì ‡¶´‡¶æ‡¶á‡¶≤
    for audio in soup.find_all('audio'):
        if audio.get('src'):
            resources.append(audio['src'])
        for source in audio.find_all('source', src=True):
            resources.append(source['src'])
    
    # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶Æ‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶´‡¶æ‡¶á‡¶≤
    for link in soup.find_all('a', href=True):
        href = link['href']
        full_url = urljoin(url, href)
        
        # ‡¶´‡¶æ‡¶á‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
        parsed_href = urlparse(href)
        path = parsed_href.path.lower()
        
        # ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ (‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡¶π) ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        file_extensions = [
            # ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶´‡¶æ‡¶á‡¶≤
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.rtf',
            # ‡¶õ‡¶¨‡¶ø ‡¶´‡¶æ‡¶á‡¶≤
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico',
            # ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶´‡¶æ‡¶á‡¶≤
            '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.mkv',
            # ‡¶Ö‡¶°‡¶ø‡¶ì ‡¶´‡¶æ‡¶á‡¶≤
            '.mp3', '.wav', '.ogg', '.m4a', '.aac', '.flac',
            # ‡¶ï‡ßã‡¶° ‡¶´‡¶æ‡¶á‡¶≤
            '.css', '.js', '.json', '.xml', '.php', '.py', '.java', '.cpp', '.c',
            # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶á‡¶≤
            '.zip', '.rar', '.tar', '.gz', '.7z',
            # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤ (‡¶°‡¶ü ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ)
            '.htaccess', '.htpasswd', '.env', '.gitignore', '.gitattributes',
            # ‡¶ï‡¶®‡¶´‡¶ø‡¶ó ‡¶´‡¶æ‡¶á‡¶≤
            '.conf', '.config', '.ini', '.cfg', '.yaml', '.yml'
        ]
        
        # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤ ‡¶ö‡ßá‡¶ï (.‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ ‡¶´‡¶æ‡¶á‡¶≤)
        filename = os.path.basename(path)
        is_hidden_file = filename.startswith('.') and len(filename) > 1
        
        # ‡¶´‡¶æ‡¶á‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶Ö‡¶•‡¶¨‡¶æ ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶´‡¶æ‡¶á‡¶≤ ‡¶π‡¶≤‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        has_extension = any(path.endswith(ext) for ext in file_extensions)
        
        if (BASE_URL in full_url and full_url not in visited_urls and 
            (has_extension or is_hidden_file or not '.' in filename)):
            resources.append(href)
    
    # ‡¶∏‡¶¨ ‡¶è‡¶≤‡¶ø‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá src, href ‡¶è‡¶¨‡¶Ç data-* ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶ü ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
    for element in soup.find_all():
        # src ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶ü
        if element.get('src'):
            resources.append(element['src'])
        
        # href ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶ü
        if element.get('href') and element.name != 'a':  # a ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶Ü‡¶ó‡ßá‡¶á ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá
            resources.append(element['href'])
        
        # data-src ‡¶¨‡¶æ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø data ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶ü
        for attr in element.attrs:
            if attr.startswith('data-') and ('src' in attr or 'url' in attr):
                resources.append(element[attr])
    
    # ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
    for resource in resources:
        if not resource or resource in ['/', '#', 'javascript:void(0)']:
            continue
            
        full_resource_url = urljoin(url, resource)
        
        # ‡¶è‡¶ï‡¶á ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶® ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
        if not full_resource_url.startswith(BASE_URL):
            continue
        
        # vendor ‡¶´‡ßã‡¶≤‡ßç‡¶°‡¶æ‡¶∞ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶®
        if should_exclude_url(full_resource_url):
            print(f"‚è≠Ô∏è Skipping vendor resource: {full_resource_url}")
            continue
        
        # ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ø ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        resource_dir = create_directory_structure(full_resource_url, dest_dir)
        resource_filename = get_filename_from_url(full_resource_url)
        resource_path = os.path.join(resource_dir, resource_filename)
        
        if not os.path.exists(resource_path):
            if download_file(full_resource_url, resource_path):
                # ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶Æ ‡¶¨‡¶ø‡¶∞‡¶§‡¶ø
                delay = random.uniform(1.5, 4.0)
                print(f"‚è±Ô∏è Sleeping for {delay:.2f} seconds...")
                time.sleep(delay)
        
        # ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø HTML ‡¶™‡ßá‡¶ú ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∞‡¶ø‡¶ï‡¶æ‡¶∞‡ßç‡¶∏‡¶ø‡¶≠‡¶≠‡¶æ‡¶¨‡ßá ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        if (resource_filename.endswith('.html') or 
            resource_filename == 'index.html' or 
            '.' not in resource_filename):
            download(full_resource_url, dest_dir)

# ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
print(f"üöÄ Starting download from: {BASE_URL}")
print(f"üìÅ Download directory: {DOWNLOAD_DIR}")
download(BASE_URL, DOWNLOAD_DIR)
print("üéâ Download completed!")